{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jthomazo/Archives/01_Projets/02_AIM/AIE6/11_Midterm_Challenge/.venv_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import List\n",
    "from document_loader import load_document_with_unstructured, split_document_with_unstructured\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"data/BD_PuppiesForDummies.pdf\"\n",
    "PUPPIES_START_PAGE = 26\n",
    "PUPPIES_END_PAGE = 403\n",
    "OUTPUT_FILE_PKL = \"preprocessed_chunks.pkl\"\n",
    "OUTPUT_FILE_JSON = \"preprocessed_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 21:26:00,869 - INFO - Loading PDF with Unstructured.io: data/BD_PuppiesForDummies.pdf, pages 26 to 403\n",
      "2025-05-11 21:26:09,417 - INFO - Extraction completed: 6089 raw elements extracted\n",
      "2025-05-11 21:26:09,426 - INFO - Conversion completed: 4129 LangChain documents created after filtering\n",
      "2025-05-11 21:26:09,431 - INFO - Splitting 4129 documents into chunks with Unstructured.io\n",
      "2025-05-11 21:26:09,470 - INFO - Splitting completed: 461 chunks created\n",
      "2025-05-11 21:26:09,471 - INFO - Nombre de chunks générés: 461\n",
      "2025-05-11 21:26:09,471 - INFO - Sauvegarde au format pickle dans preprocessed_chunks.pkl...\n",
      "2025-05-11 21:26:09,472 - INFO - Sauvegarde au format JSON dans preprocessed_chunks.json...\n",
      "2025-05-11 21:26:09,476 - INFO - Prétraitement terminé avec succès!\n",
      "2025-05-11 21:26:09,477 - INFO - Statistiques:\n",
      "2025-05-11 21:26:09,477 - INFO - - Nombre total de chunks: 461\n",
      "2025-05-11 21:26:09,477 - INFO - - Nombre approximatif de tokens: 164067\n",
      "2025-05-11 21:26:09,477 - INFO - - Taille du fichier pickle: 0.73 MB\n",
      "2025-05-11 21:26:09,477 - INFO - - Taille du fichier JSON: 0.81 MB\n"
     ]
    }
   ],
   "source": [
    "# preprocess_and_save.py\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.documents.elements import Text, ElementMetadata\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"data/BD_PuppiesForDummies.pdf\"\n",
    "PUPPIES_START_PAGE = 26\n",
    "PUPPIES_END_PAGE = 403\n",
    "OUTPUT_FILE_PKL = \"preprocessed_chunks.pkl\"\n",
    "OUTPUT_FILE_JSON = \"preprocessed_chunks.json\"\n",
    "\n",
    "def load_document_with_unstructured(pdf_path: str, start_page: Optional[int] = None, end_page: Optional[int] = None) -> List[Document]:\n",
    "    \"\"\" Loads a PDF document with Unstructured.io and converts it to LangChain format \"\"\"\n",
    "    logger.info(f\"Loading PDF with Unstructured.io: {pdf_path}, pages {start_page} to {end_page}\")\n",
    "\n",
    "    page_range = None\n",
    "    if start_page is not None and end_page is not None:\n",
    "        page_range = list(range(start_page, end_page + 1))\n",
    "\n",
    "    try:\n",
    "        extracted_elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            strategy=\"fast\",\n",
    "            include_page_breaks=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while partitioning PDF {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    logger.info(f\"Extraction completed: {len(extracted_elements)} raw elements extracted\")\n",
    "\n",
    "    documents = []\n",
    "    for element in extracted_elements:\n",
    "        # Ignore empty or non-textual elements\n",
    "        if not hasattr(element, 'text') or not element.text.strip():\n",
    "            continue\n",
    "\n",
    "        current_page_number = getattr(element.metadata, 'page_number', None)\n",
    "\n",
    "        # Page filtering\n",
    "        if start_page is not None and current_page_number is not None and current_page_number < start_page:\n",
    "            continue\n",
    "        if end_page is not None and current_page_number is not None and current_page_number > end_page:\n",
    "            continue\n",
    "        \n",
    "        # Build metadata for LangChain Document\n",
    "        metadata = {\n",
    "            \"source\": pdf_path,\n",
    "            \"page\": current_page_number,\n",
    "            \"category\": str(type(element).__name__),\n",
    "            \"id\": str(element.id) if hasattr(element, \"id\") else str(uuid4()),\n",
    "        }\n",
    "        if hasattr(element.metadata, 'filename'):\n",
    "            metadata[\"filename\"] = element.metadata.filename\n",
    "        if hasattr(element.metadata, 'filetype'):\n",
    "            metadata[\"filetype\"] = element.metadata.filetype\n",
    "        if hasattr(element.metadata, 'parent_id') and element.metadata.parent_id is not None:\n",
    "            metadata[\"parent_id\"] = str(element.metadata.parent_id)\n",
    "\n",
    "        documents.append(Document(\n",
    "            page_content=element.text,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "\n",
    "    logger.info(f\"Conversion completed: {len(documents)} LangChain documents created after filtering\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_document_with_unstructured(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\" Splits documents into chunks using Unstructured.io features \"\"\"\n",
    "    if not documents:\n",
    "        logger.warning(\"No documents to split.\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Splitting {len(documents)} documents into chunks with Unstructured.io\")\n",
    "\n",
    "    chunked_langchain_documents = []\n",
    "\n",
    "    # Recreate Unstructured elements from LangChain documents\n",
    "    unstructured_elements_for_chunking = []\n",
    "\n",
    "    valid_categories = [\"NarrativeText\", \"ListItem\", \"Title\"]\n",
    "\n",
    "    for doc in documents:\n",
    "        if doc.metadata.get(\"category\") not in valid_categories:\n",
    "            continue\n",
    "\n",
    "        if len(doc.page_content.strip()) < 50:\n",
    "            continue\n",
    "            \n",
    "        # Clean the text\n",
    "        cleaned_text = doc.page_content\n",
    "        # Replace references like \"FIGURE XX-X:\" with an empty string\n",
    "        cleaned_text = re.sub(r'FIGURE \\d+-\\d+:', '', cleaned_text)\n",
    "        # Replace \\xad (soft hyphen) with an empty string to fix broken text\n",
    "        cleaned_text = cleaned_text.replace('\\xad', ' ')\n",
    "        # Reassign the cleaned text to the document\n",
    "        doc.page_content = cleaned_text\n",
    "        \n",
    "        # Create an ElementMetadata object from the metadata dictionary\n",
    "        element_meta = ElementMetadata()\n",
    "        if doc.metadata.get(\"filename\"):\n",
    "            element_meta.filename = doc.metadata.get(\"filename\")\n",
    "        if doc.metadata.get(\"filetype\"):\n",
    "            element_meta.filetype = doc.metadata.get(\"filetype\")\n",
    "        if doc.metadata.get(\"page\"):\n",
    "            element_meta.page_number = doc.metadata.get(\"page\")\n",
    "        \n",
    "        # Create the Text element with appropriate metadata\n",
    "        element = Text(\n",
    "            text=doc.page_content, \n",
    "            metadata=element_meta, \n",
    "            element_id=doc.metadata.get(\"id\", str(uuid4()))\n",
    "        )\n",
    "        unstructured_elements_for_chunking.append(element)\n",
    "\n",
    "    if not unstructured_elements_for_chunking:\n",
    "        logger.warning(\"No Unstructured elements could be created from LangChain documents.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Apply standard chunking\n",
    "        chunks = chunk_elements(\n",
    "            elements=unstructured_elements_for_chunking,\n",
    "            max_characters=1800,\n",
    "            new_after_n_chars=1500,  # To avoid chunks that are too long\n",
    "            overlap=400,  # 400 characters of overlap\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while chunking elements: {str(e)}. Returning unsplit documents.\")\n",
    "        return documents\n",
    "\n",
    "    # Convert chunks to LangChain Documents\n",
    "    for i, chunk_element in enumerate(chunks):\n",
    "        # Chunks are Element objects (or CompositeElement)\n",
    "        page = getattr(chunk_element.metadata, 'page_number', 0)\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": documents[0].metadata.get(\"source\", \"\"),\n",
    "            \"page\": page,\n",
    "            \"chunk_index\": i,\n",
    "            \"id\": str(chunk_element.id) if hasattr(chunk_element, \"id\") else str(uuid4()),\n",
    "            \"word_count\": len(chunk_element.text.split()) if hasattr(chunk_element, 'text') else 0,\n",
    "        }\n",
    "        # Estimate the number of tokens\n",
    "        metadata[\"token_count_approx\"] = int(metadata[\"word_count\"] * 1.3)\n",
    "        \n",
    "        chunked_langchain_documents.append(Document(\n",
    "            page_content=chunk_element.text if hasattr(chunk_element, 'text') else \"\",\n",
    "            metadata=metadata\n",
    "        ))\n",
    "\n",
    "    logger.info(f\"Splitting completed: {len(chunked_langchain_documents)} chunks created\")\n",
    "    return chunked_langchain_documents\n",
    "\n",
    "def preprocess_and_save():\n",
    "    \"\"\"Prétraite le document et sauvegarde les chunks\"\"\"\n",
    "    # Charger le document\n",
    "    documents = load_document_with_unstructured(\n",
    "        pdf_path=PDF_PATH,\n",
    "        start_page=PUPPIES_START_PAGE,\n",
    "        end_page=PUPPIES_END_PAGE\n",
    "    )\n",
    "    \n",
    "    # Diviser en chunks\n",
    "    chunks = split_document_with_unstructured(documents)\n",
    "    logger.info(f\"Nombre de chunks générés: {len(chunks)}\")\n",
    "    \n",
    "    # Conversion des Documents en dictionnaires sérialisables\n",
    "    serializable_chunks = []\n",
    "    for chunk in chunks:\n",
    "        serializable_chunks.append({\n",
    "            \"page_content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        })\n",
    "    \n",
    "    # Sauvegarde en format pickle\n",
    "    logger.info(f\"Sauvegarde au format pickle dans {OUTPUT_FILE_PKL}...\")\n",
    "    with open(OUTPUT_FILE_PKL, 'wb') as f:\n",
    "        pickle.dump(serializable_chunks, f)\n",
    "    \n",
    "    # Sauvegarde en format JSON (plus portable)\n",
    "    logger.info(f\"Sauvegarde au format JSON dans {OUTPUT_FILE_JSON}...\")\n",
    "    with open(OUTPUT_FILE_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_chunks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(\"Prétraitement terminé avec succès!\")\n",
    "    \n",
    "    # Afficher des statistiques\n",
    "    total_tokens = sum(chunk[\"metadata\"].get(\"token_count_approx\", 0) for chunk in serializable_chunks)\n",
    "    logger.info(f\"Statistiques:\")\n",
    "    logger.info(f\"- Nombre total de chunks: {len(serializable_chunks)}\")\n",
    "    logger.info(f\"- Nombre approximatif de tokens: {total_tokens}\")\n",
    "    logger.info(f\"- Taille du fichier pickle: {os.path.getsize(OUTPUT_FILE_PKL) / (1024*1024):.2f} MB\")\n",
    "    logger.info(f\"- Taille du fichier JSON: {os.path.getsize(OUTPUT_FILE_JSON) / (1024*1024):.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
