{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jthomazo/Archives/01_Projets/02_AIM/AIE6/11_Midterm_Challenge/.venv_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-12 14:38:44,195 - INFO - PyTorch version 2.7.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "import nest_asyncio\n",
    "from typing import List, Dict\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from document_loader import load_document_with_unstructured, split_document_with_unstructured\n",
    "from embedding_models import SnowflakeArcticEmbedModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: mps\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio to allow asynchronous code\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "TOTAL_EXAMPLES = 150  # Total number of examples desired\n",
    "TRAIN_RATIO = 0.7     # 105 examples\n",
    "VAL_RATIO = 0.15      # 22 examples\n",
    "TEST_RATIO = 0.15     # 23 examples\n",
    "# Load env\n",
    "load_dotenv()\n",
    "\n",
    "# Request token interactively\n",
    "hf_token = getpass(\"Enter your Hugging Face token: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"Charge un document PDF à partir d'un chemin spécifique\"\"\"\n",
    "    documents = []\n",
    "    if os.path.isfile(pdf_path) and pdf_path.endswith('.pdf'):\n",
    "        doc_pages = load_document_with_unstructured(pdf_path)\n",
    "        documents.extend(doc_pages)\n",
    "    elif os.path.isdir(pdf_path):\n",
    "        for filename in os.listdir(pdf_path):\n",
    "            if filename.endswith('.pdf'):\n",
    "                file_path = os.path.join(pdf_path, filename)\n",
    "                doc_pages = load_document_with_unstructured(file_path)\n",
    "                documents.extend(doc_pages)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Le chemin spécifié n'existe pas ou n'est pas un fichier PDF: {pdf_path}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def prepare_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Split documents into chunks and add unique IDs\"\"\"\n",
    "    # Split documents\n",
    "    chunked_documents = split_document_with_unstructured(documents)\n",
    "    \n",
    "    # Add unique IDs\n",
    "    id_set = set()\n",
    "    for document in chunked_documents:\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        while doc_id in id_set:\n",
    "            doc_id = str(uuid.uuid4())\n",
    "        id_set.add(doc_id)\n",
    "        document.metadata[\"id\"] = doc_id\n",
    "    \n",
    "    return chunked_documents\n",
    "\n",
    "\n",
    "async def create_questions(documents, n_questions_per_doc=2):\n",
    "    \"\"\"Generate questions for each document chunk using ChatGPT\"\"\"\n",
    "    \n",
    "    # Question generation model configuration\n",
    "    qa_chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    # Create prompt template for generating questions\n",
    "    qa_prompt = \"\"\"\\\n",
    "    Given the following context about puppy care and training, generate questions that an owner might ask about this information.\n",
    "    \n",
    "    You should generate {n_questions} questions that must be presented in the following format:\n",
    "    \n",
    "    1. QUESTION #1\n",
    "    2. QUESTION #2\n",
    "    ...\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)\n",
    "    question_generation_chain = qa_prompt_template | qa_chat_model\n",
    "    \n",
    "    questions = {}\n",
    "    relevant_docs = {}\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(total=len(documents), desc=\"Generating questions for documents\")\n",
    "    \n",
    "    for document in documents:\n",
    "        context = document.page_content\n",
    "        doc_id = document.metadata[\"id\"]\n",
    "        \n",
    "        try:\n",
    "            chain_output = await question_generation_chain.ainvoke({\n",
    "                \"context\": context,\n",
    "                \"n_questions\": n_questions_per_doc\n",
    "            })\n",
    "            \n",
    "            generated_questions = chain_output.content\n",
    "            question_lines = generated_questions.strip().split(\"\\n\")\n",
    "            \n",
    "            for q_line in question_lines:\n",
    "                if not q_line.strip():\n",
    "                    continue\n",
    "                \n",
    "                q_text = re.sub(r'^\\d+\\.\\s+', '', q_line.strip())\n",
    "                q_id = str(uuid.uuid4())\n",
    "                \n",
    "                questions[q_id] = q_text\n",
    "                relevant_docs[q_id] = [doc_id]\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating questions for document {doc_id}: {e}\")\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return questions, relevant_docs\n",
    "\n",
    "\n",
    "def split_dataset(documents, questions, relevant_docs, total_examples=150):\n",
    "    \"\"\"Split the dataset into training, validation, and test sets\"\"\"\n",
    "    # Calculate number of examples per split\n",
    "    n_train = int(total_examples * TRAIN_RATIO)\n",
    "    n_val = int(total_examples * VAL_RATIO)\n",
    "    n_test = total_examples - n_train - n_val\n",
    "    \n",
    "    # Get question IDs\n",
    "    question_ids = list(questions.keys())\n",
    "    \n",
    "    # Make sure we have enough questions\n",
    "    if len(question_ids) < total_examples:\n",
    "        print(f\"Warning: Only {len(question_ids)} questions available, less than the requested {total_examples}\")\n",
    "        n_train = int(len(question_ids) * TRAIN_RATIO)\n",
    "        n_val = int(len(question_ids) * VAL_RATIO)\n",
    "        n_test = len(question_ids) - n_train - n_val\n",
    "    else:\n",
    "        # Truncate to total_examples\n",
    "        question_ids = question_ids[:total_examples]\n",
    "    \n",
    "    # Split question IDs\n",
    "    train_ids = question_ids[:n_train]\n",
    "    val_ids = question_ids[n_train:n_train+n_val]\n",
    "    test_ids = question_ids[n_train+n_val:]\n",
    "    \n",
    "    # Function to filter dataset by question IDs\n",
    "    def filter_dataset(ids):\n",
    "        filtered_questions = {q_id: questions[q_id] for q_id in ids}\n",
    "        filtered_relevant_docs = {q_id: relevant_docs[q_id] for q_id in ids}\n",
    "        \n",
    "        # Get all document IDs needed for this split\n",
    "        doc_ids = set()\n",
    "        for rel_docs in filtered_relevant_docs.values():\n",
    "            doc_ids.update(rel_docs)\n",
    "        \n",
    "        # Filter documents\n",
    "        filtered_corpus = {}\n",
    "        for doc in documents:\n",
    "            if doc.metadata[\"id\"] in doc_ids:\n",
    "                filtered_corpus[doc.metadata[\"id\"]] = doc.page_content\n",
    "        \n",
    "        return {\n",
    "            \"questions\": filtered_questions,\n",
    "            \"relevant_contexts\": filtered_relevant_docs,\n",
    "            \"corpus\": filtered_corpus\n",
    "        }\n",
    "    \n",
    "    # Create splits\n",
    "    train_dataset = filter_dataset(train_ids)\n",
    "    val_dataset = filter_dataset(val_ids)\n",
    "    test_dataset = filter_dataset(test_ids)\n",
    "    \n",
    "    print(f\"Sets created - Train: {len(train_dataset['questions'])} questions, Val: {len(val_dataset['questions'])} questions, Test: {len(test_dataset['questions'])} questions\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def fine_tune_embedding_model(train_dataset, val_dataset, model_name=\"Snowflake/snowflake-arctic-embed-l\"):\n",
    "    \"\"\"Fine-tune the embedding model\"\"\"\n",
    "    # Load base model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model = model.to(device)\n",
    "    print(f\"Model loaded and transferred to {device}\")\n",
    "    \n",
    "    # Prepare training examples\n",
    "    corpus = train_dataset['corpus']\n",
    "    queries = train_dataset['questions']\n",
    "    relevant_docs = train_dataset['relevant_contexts']\n",
    "    \n",
    "    examples = []\n",
    "    for query_id, query in queries.items():\n",
    "        doc_id = relevant_docs[query_id][0]\n",
    "        text = corpus[doc_id]\n",
    "        example = InputExample(texts=[query, text])\n",
    "        examples.append(example)\n",
    "    \n",
    "    # Prepare data loader\n",
    "    loader = DataLoader(examples, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Configure loss function - MatryoshkaLoss for multi-dimensional embeddings\n",
    "    matryoshka_dimensions = [1024, 512, 256, 128, 64]  # Adjust according to your model\n",
    "    inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "    train_loss = MatryoshkaLoss(\n",
    "        model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    "    )\n",
    "    \n",
    "    # Configure evaluator\n",
    "    val_corpus = val_dataset['corpus']\n",
    "    val_queries = val_dataset['questions']\n",
    "    val_relevant_docs = val_dataset['relevant_contexts']\n",
    "    evaluator = InformationRetrievalEvaluator(val_queries, val_corpus, val_relevant_docs)\n",
    "    \n",
    "    # Calculate warm-up steps\n",
    "    warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path='puppy_finetuned_embeddings',\n",
    "        show_progress_bar=True,\n",
    "        evaluator=evaluator,\n",
    "        evaluation_steps=50\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def push_to_huggingface(model, username, model_name_prefix=\"puppy-embed\"):\n",
    "    \"\"\"Push the fine-tuned model to Hugging Face Hub\"\"\"\n",
    "    # Generate unique model name\n",
    "    model_name = f\"{username}/{model_name_prefix}-{str(uuid.uuid4())[:8]}\"\n",
    "    \n",
    "    # Push to Hub\n",
    "    model.push_to_hub(model_name)\n",
    "    print(f\"Model pushed to Hugging Face Hub: {model_name}\")\n",
    "    \n",
    "    return model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR=\"puppy_finetuned_embeddings\"\n",
    "HF_USERNAME=\"JTh34\"\n",
    "BASE_MODEL=\"Snowflake/snowflake-arctic-embed-l\"\n",
    "PDF_PATH = \"data/BD_PuppiesForDummies.pdf\"\n",
    "# 1. Load documents\n",
    "print(\"Loading documents...\")\n",
    "raw_documents = load_documents(PDF_PATH)\n",
    "print(f\"Loaded {len(raw_documents)} pages of raw documents\")\n",
    "\n",
    "# 2. Prepare documents\n",
    "print(\"Preparing documents...\")\n",
    "chunked_documents = prepare_documents(raw_documents)\n",
    "print(f\"Created {len(chunked_documents)} document chunks\")\n",
    "\n",
    "# 3. Generate questions\n",
    "print(\"Generating questions...\")\n",
    "questions, relevant_docs = await create_questions(chunked_documents)\n",
    "print(f\"Generated {len(questions)} questions\")\n",
    "\n",
    "# 4. Split dataset\n",
    "print(\"Splitting dataset...\")\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(\n",
    "    chunked_documents, questions, relevant_docs, TOTAL_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save datasets\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"train_dataset.json\"), \"w\") as f:\n",
    "    json.dump(train_dataset, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"val_dataset.json\"), \"w\") as f:\n",
    "    json.dump(val_dataset, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_dataset.json\"), \"w\") as f:\n",
    "    json.dump(test_dataset, f)\n",
    "print(f\"Datasets saved in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR=\"puppy_finetuned_embeddings\"\n",
    "HF_USERNAME=\"JTh34\"\n",
    "BASE_MODEL=\"Snowflake/snowflake-arctic-embed-l\"\n",
    "PDF_PATH = \"data/BD_PuppiesForDummies.pdf\"\n",
    "# recharger les datasets\n",
    "with open(os.path.join(OUTPUT_DIR, \"train_dataset.json\"), \"r\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"val_dataset.json\"), \"r\") as f:\n",
    "    val_dataset = json.load(f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_dataset.json\"), \"r\") as f:\n",
    "    test_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 14:41:34,103 - INFO - Use pytorch device_name: mps\n",
      "2025-05-12 14:41:34,106 - INFO - Load pretrained SentenceTransformer: Snowflake/snowflake-arctic-embed-l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 14:41:39,894 - INFO - 1 prompts are loaded, with the keys: ['query']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and transferred to mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 08:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy@1</th>\n",
       "      <th>Cosine Accuracy@3</th>\n",
       "      <th>Cosine Accuracy@5</th>\n",
       "      <th>Cosine Accuracy@10</th>\n",
       "      <th>Cosine Precision@1</th>\n",
       "      <th>Cosine Precision@3</th>\n",
       "      <th>Cosine Precision@5</th>\n",
       "      <th>Cosine Precision@10</th>\n",
       "      <th>Cosine Recall@1</th>\n",
       "      <th>Cosine Recall@3</th>\n",
       "      <th>Cosine Recall@5</th>\n",
       "      <th>Cosine Recall@10</th>\n",
       "      <th>Cosine Ndcg@10</th>\n",
       "      <th>Cosine Mrr@10</th>\n",
       "      <th>Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775166</td>\n",
       "      <td>0.702273</td>\n",
       "      <td>0.702273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775166</td>\n",
       "      <td>0.702273</td>\n",
       "      <td>0.702273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780848</td>\n",
       "      <td>0.711364</td>\n",
       "      <td>0.711364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819311</td>\n",
       "      <td>0.763312</td>\n",
       "      <td>0.763312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843933</td>\n",
       "      <td>0.793182</td>\n",
       "      <td>0.793182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843933</td>\n",
       "      <td>0.793182</td>\n",
       "      <td>0.793182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860709</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.815909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.823485</td>\n",
       "      <td>0.823485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.823485</td>\n",
       "      <td>0.823485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.823485</td>\n",
       "      <td>0.823485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 14:43:00,378 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 0.9433962264150944 after 50 steps:\n",
      "2025-05-12 14:43:04,976 - INFO - Queries: 22\n",
      "2025-05-12 14:43:04,979 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:43:04,986 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:43:04,986 - INFO - Accuracy@1: 54.55%\n",
      "2025-05-12 14:43:04,986 - INFO - Accuracy@3: 81.82%\n",
      "2025-05-12 14:43:04,987 - INFO - Accuracy@5: 90.91%\n",
      "2025-05-12 14:43:04,987 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:43:04,987 - INFO - Precision@1: 54.55%\n",
      "2025-05-12 14:43:04,987 - INFO - Precision@3: 27.27%\n",
      "2025-05-12 14:43:04,987 - INFO - Precision@5: 18.18%\n",
      "2025-05-12 14:43:04,987 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:43:04,988 - INFO - Recall@1: 54.55%\n",
      "2025-05-12 14:43:04,988 - INFO - Recall@3: 81.82%\n",
      "2025-05-12 14:43:04,988 - INFO - Recall@5: 90.91%\n",
      "2025-05-12 14:43:04,988 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:43:04,988 - INFO - MRR@10: 0.7023\n",
      "2025-05-12 14:43:04,988 - INFO - NDCG@10: 0.7752\n",
      "2025-05-12 14:43:04,989 - INFO - MAP@100: 0.7023\n",
      "2025-05-12 14:43:04,999 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:43:31,854 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 1.0 after 53 steps:\n",
      "2025-05-12 14:43:33,854 - INFO - Queries: 22\n",
      "2025-05-12 14:43:33,859 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:43:33,862 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:43:33,862 - INFO - Accuracy@1: 54.55%\n",
      "2025-05-12 14:43:33,863 - INFO - Accuracy@3: 81.82%\n",
      "2025-05-12 14:43:33,863 - INFO - Accuracy@5: 90.91%\n",
      "2025-05-12 14:43:33,863 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:43:33,863 - INFO - Precision@1: 54.55%\n",
      "2025-05-12 14:43:33,864 - INFO - Precision@3: 27.27%\n",
      "2025-05-12 14:43:33,864 - INFO - Precision@5: 18.18%\n",
      "2025-05-12 14:43:33,864 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:43:33,864 - INFO - Recall@1: 54.55%\n",
      "2025-05-12 14:43:33,864 - INFO - Recall@3: 81.82%\n",
      "2025-05-12 14:43:33,865 - INFO - Recall@5: 90.91%\n",
      "2025-05-12 14:43:33,865 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:43:33,865 - INFO - MRR@10: 0.7023\n",
      "2025-05-12 14:43:33,865 - INFO - NDCG@10: 0.7752\n",
      "2025-05-12 14:43:33,865 - INFO - MAP@100: 0.7023\n",
      "2025-05-12 14:44:54,186 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 1.8867924528301887 after 100 steps:\n",
      "2025-05-12 14:44:55,859 - INFO - Queries: 22\n",
      "2025-05-12 14:44:55,860 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:44:55,865 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:44:55,866 - INFO - Accuracy@1: 59.09%\n",
      "2025-05-12 14:44:55,866 - INFO - Accuracy@3: 77.27%\n",
      "2025-05-12 14:44:55,867 - INFO - Accuracy@5: 90.91%\n",
      "2025-05-12 14:44:55,867 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:44:55,868 - INFO - Precision@1: 59.09%\n",
      "2025-05-12 14:44:55,868 - INFO - Precision@3: 25.76%\n",
      "2025-05-12 14:44:55,868 - INFO - Precision@5: 18.18%\n",
      "2025-05-12 14:44:55,869 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:44:55,869 - INFO - Recall@1: 59.09%\n",
      "2025-05-12 14:44:55,869 - INFO - Recall@3: 77.27%\n",
      "2025-05-12 14:44:55,870 - INFO - Recall@5: 90.91%\n",
      "2025-05-12 14:44:55,870 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:44:55,870 - INFO - MRR@10: 0.7114\n",
      "2025-05-12 14:44:55,871 - INFO - NDCG@10: 0.7808\n",
      "2025-05-12 14:44:55,871 - INFO - MAP@100: 0.7114\n",
      "2025-05-12 14:44:55,878 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:45:12,982 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 2.0 after 106 steps:\n",
      "2025-05-12 14:45:14,664 - INFO - Queries: 22\n",
      "2025-05-12 14:45:14,665 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:45:14,671 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:45:14,672 - INFO - Accuracy@1: 68.18%\n",
      "2025-05-12 14:45:14,672 - INFO - Accuracy@3: 77.27%\n",
      "2025-05-12 14:45:14,672 - INFO - Accuracy@5: 90.91%\n",
      "2025-05-12 14:45:14,672 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:45:14,673 - INFO - Precision@1: 68.18%\n",
      "2025-05-12 14:45:14,673 - INFO - Precision@3: 25.76%\n",
      "2025-05-12 14:45:14,673 - INFO - Precision@5: 18.18%\n",
      "2025-05-12 14:45:14,673 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:45:14,674 - INFO - Recall@1: 68.18%\n",
      "2025-05-12 14:45:14,674 - INFO - Recall@3: 77.27%\n",
      "2025-05-12 14:45:14,674 - INFO - Recall@5: 90.91%\n",
      "2025-05-12 14:45:14,675 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:45:14,675 - INFO - MRR@10: 0.7633\n",
      "2025-05-12 14:45:14,678 - INFO - NDCG@10: 0.8193\n",
      "2025-05-12 14:45:14,679 - INFO - MAP@100: 0.7633\n",
      "2025-05-12 14:45:14,683 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:46:32,259 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 2.830188679245283 after 150 steps:\n",
      "2025-05-12 14:46:33,936 - INFO - Queries: 22\n",
      "2025-05-12 14:46:33,936 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:46:33,938 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:46:33,939 - INFO - Accuracy@1: 68.18%\n",
      "2025-05-12 14:46:33,939 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:46:33,939 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:46:33,939 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:46:33,940 - INFO - Precision@1: 68.18%\n",
      "2025-05-12 14:46:33,940 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:46:33,940 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:46:33,940 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:46:33,940 - INFO - Recall@1: 68.18%\n",
      "2025-05-12 14:46:33,940 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:46:33,940 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:46:33,941 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:46:33,941 - INFO - MRR@10: 0.7932\n",
      "2025-05-12 14:46:33,941 - INFO - NDCG@10: 0.8439\n",
      "2025-05-12 14:46:33,941 - INFO - MAP@100: 0.7932\n",
      "2025-05-12 14:46:33,943 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:46:52,252 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 3.0 after 159 steps:\n",
      "2025-05-12 14:46:53,943 - INFO - Queries: 22\n",
      "2025-05-12 14:46:53,945 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:46:53,946 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:46:53,946 - INFO - Accuracy@1: 68.18%\n",
      "2025-05-12 14:46:53,947 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:46:53,947 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:46:53,947 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:46:53,947 - INFO - Precision@1: 68.18%\n",
      "2025-05-12 14:46:53,947 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:46:53,948 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:46:53,948 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:46:53,948 - INFO - Recall@1: 68.18%\n",
      "2025-05-12 14:46:53,948 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:46:53,948 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:46:53,949 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:46:53,949 - INFO - MRR@10: 0.7932\n",
      "2025-05-12 14:46:53,949 - INFO - NDCG@10: 0.8439\n",
      "2025-05-12 14:46:53,949 - INFO - MAP@100: 0.7932\n",
      "2025-05-12 14:47:57,608 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 3.7735849056603774 after 200 steps:\n",
      "2025-05-12 14:47:59,247 - INFO - Queries: 22\n",
      "2025-05-12 14:47:59,248 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:47:59,252 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:47:59,253 - INFO - Accuracy@1: 72.73%\n",
      "2025-05-12 14:47:59,253 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:47:59,254 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:47:59,254 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:47:59,254 - INFO - Precision@1: 72.73%\n",
      "2025-05-12 14:47:59,255 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:47:59,255 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:47:59,256 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:47:59,256 - INFO - Recall@1: 72.73%\n",
      "2025-05-12 14:47:59,256 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:47:59,257 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:47:59,257 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:47:59,257 - INFO - MRR@10: 0.8159\n",
      "2025-05-12 14:47:59,258 - INFO - NDCG@10: 0.8607\n",
      "2025-05-12 14:47:59,258 - INFO - MAP@100: 0.8159\n",
      "2025-05-12 14:47:59,267 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:48:25,411 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 4.0 after 212 steps:\n",
      "2025-05-12 14:48:27,185 - INFO - Queries: 22\n",
      "2025-05-12 14:48:27,186 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:48:27,202 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:48:27,202 - INFO - Accuracy@1: 72.73%\n",
      "2025-05-12 14:48:27,204 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:48:27,204 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:48:27,204 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:48:27,205 - INFO - Precision@1: 72.73%\n",
      "2025-05-12 14:48:27,205 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:48:27,205 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:48:27,205 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:48:27,205 - INFO - Recall@1: 72.73%\n",
      "2025-05-12 14:48:27,206 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:48:27,206 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:48:27,206 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:48:27,206 - INFO - MRR@10: 0.8235\n",
      "2025-05-12 14:48:27,207 - INFO - NDCG@10: 0.8667\n",
      "2025-05-12 14:48:27,207 - INFO - MAP@100: 0.8235\n",
      "2025-05-12 14:48:27,209 - INFO - Save model to puppy_finetuned_embeddings\n",
      "2025-05-12 14:49:28,722 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 4.716981132075472 after 250 steps:\n",
      "2025-05-12 14:49:30,398 - INFO - Queries: 22\n",
      "2025-05-12 14:49:30,400 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:49:30,405 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:49:30,406 - INFO - Accuracy@1: 72.73%\n",
      "2025-05-12 14:49:30,406 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:49:30,406 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:49:30,407 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:49:30,408 - INFO - Precision@1: 72.73%\n",
      "2025-05-12 14:49:30,408 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:49:30,409 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:49:30,409 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:49:30,409 - INFO - Recall@1: 72.73%\n",
      "2025-05-12 14:49:30,410 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:49:30,410 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:49:30,411 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:49:30,411 - INFO - MRR@10: 0.8235\n",
      "2025-05-12 14:49:30,411 - INFO - NDCG@10: 0.8667\n",
      "2025-05-12 14:49:30,412 - INFO - MAP@100: 0.8235\n",
      "2025-05-12 14:49:51,825 - INFO - Information Retrieval Evaluation of the model on the  dataset in epoch 5.0 after 265 steps:\n",
      "2025-05-12 14:49:53,471 - INFO - Queries: 22\n",
      "2025-05-12 14:49:53,471 - INFO - Corpus: 12\n",
      "\n",
      "2025-05-12 14:49:53,473 - INFO - Score-Function: cosine\n",
      "2025-05-12 14:49:53,474 - INFO - Accuracy@1: 72.73%\n",
      "2025-05-12 14:49:53,475 - INFO - Accuracy@3: 86.36%\n",
      "2025-05-12 14:49:53,475 - INFO - Accuracy@5: 95.45%\n",
      "2025-05-12 14:49:53,476 - INFO - Accuracy@10: 100.00%\n",
      "2025-05-12 14:49:53,476 - INFO - Precision@1: 72.73%\n",
      "2025-05-12 14:49:53,476 - INFO - Precision@3: 28.79%\n",
      "2025-05-12 14:49:53,477 - INFO - Precision@5: 19.09%\n",
      "2025-05-12 14:49:53,477 - INFO - Precision@10: 10.00%\n",
      "2025-05-12 14:49:53,478 - INFO - Recall@1: 72.73%\n",
      "2025-05-12 14:49:53,478 - INFO - Recall@3: 86.36%\n",
      "2025-05-12 14:49:53,478 - INFO - Recall@5: 95.45%\n",
      "2025-05-12 14:49:53,478 - INFO - Recall@10: 100.00%\n",
      "2025-05-12 14:49:53,479 - INFO - MRR@10: 0.8235\n",
      "2025-05-12 14:49:53,479 - INFO - NDCG@10: 0.8667\n",
      "2025-05-12 14:49:53,479 - INFO - MAP@100: 0.8235\n"
     ]
    }
   ],
   "source": [
    "# 5. Fine-tune the model\n",
    "print(\"Fine-tuning the model...\")\n",
    "model = fine_tune_embedding_model(train_dataset, val_dataset, BASE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 14:53:10,841 - INFO - Save model to /var/folders/8h/kl800c1j6hjc9xt9lm_1bhph0000gn/T/tmpkkhpgs1t\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [05:45<00:00, 3.87MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to Hugging Face Hub: JTh34/puppy-embed-8985966a\n",
      "Use this model name in your embedding_models.py: JTh34/puppy-embed-8985966a\n"
     ]
    }
   ],
   "source": [
    "# 6. Push to Hugging Face (optional)\n",
    "\n",
    "print(\"Pushing model to Hugging Face...\")\n",
    "\n",
    "login(token=hf_token)\n",
    "    \n",
    "model_name = push_to_huggingface(model, HF_USERNAME)\n",
    "print(f\"Use this model name in your embedding_models.py: {model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
